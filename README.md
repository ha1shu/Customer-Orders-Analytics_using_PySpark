# ğŸ§  Customer-Orders Analytics using PySpark

This project demonstrates how to analyze customer behavior and spending using **PySpark**. It includes data preprocessing, aggregations, window functions, and joins on synthetic e-commerce data.

---
## ğŸ“ Project Structure
 
â”œâ”€â”€ data/  

â”‚ â”œâ”€â”€ customers.csv ( Customer dataset with Indian entries )

â”‚ â””â”€â”€ orders.csv ( Orders dataset )  

â”œâ”€â”€ customer_order_analysis.ipynb (Main PySpark analysis notebook)  

â”œâ”€â”€ README.md # Project overview (this file)  

## ğŸ“Š Datasets Description

### ğŸ§‘â€ğŸ’¼ Customers Table

| Column            | Description                         |
|-------------------|-------------------------------------|
| customer_id       | Unique identifier for the customer  |
| name              | Full name of the customer           |
| city              | City in India                       |
| state             | State in India                      |
| country           | Country (India)                     |
| registration_date | Date of customer registration       |
| email             | Customer email                      |
| is_active         | Boolean flag (1 = active, 0 = inactive) |

### ğŸ›’ Orders Table

| Column       | Description                           |
|--------------|---------------------------------------|
| order_id     | Unique identifier for the order       |
| customer_id  | Foreign key referencing customer_id   |
| order_date   | Date when the order was placed        |
| total_amount | Total order value in INR              |
| status       | Status of the order (e.g. 'paid')     |

---

## ğŸ” Analytics Performed

- ğŸ§¾ Count of orders per customer
- ğŸ’¸ Total spending per customer
- ğŸ… Customer ranking by total spend using `dense_rank()`
- ğŸ”— Joined view of customer order count and total spend
- ğŸ“Š Sorted customer insights by spending and order frequency

---

## ğŸ›  Technologies Used

- Apache Spark (PySpark)
- Python 3
- Jupyter Notebook
- Google Cloud Platform (GCP) â€“ optional for data hosting
- Databricks (optional environment)


 
